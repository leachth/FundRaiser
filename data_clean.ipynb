{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FundRaiser  \n",
    "## Data Cleaning  \n",
    "\n",
    "Overview: This notebook contains the code for the data cleaning and aggregation of:\n",
    "* Census demographic data (2000 and 2010)\n",
    "* IRS tax data (annual zipcode level values 2008-2014)\n",
    "* Federal Election Commission data (2006-2014)  \n",
    "    -- Individual donor data with donor name, address, amount and committee ID they donated to  \n",
    "    -- Committee ID and Candidate ID linkages data  \n",
    "    -- Candidate master data (all candidate names and party linked to unique FEC candidate ID)  \n",
    "\n",
    "THL  \n",
    "last updated July 13, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1790,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1791,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in raw census data data and drop row 1, which are the descriptions of the contents, units etc., row 0 are the headers \n",
    "# 2000\n",
    "sf12000 = pd.read_csv(\"data/census data/aff_download SF1 table/DEC_00_SF1_DP1_with_ann.csv\",\n",
    "                       skiprows=[1],  dtype={'GEO.id2': object})\n",
    "# 2010\n",
    "sf12010 = pd.read_csv(\"data/census data/aff_download SF1 table/DEC_10_SF1_SF1DP1_with_ann.csv\",\n",
    "                       skiprows=[1],  dtype={'GEO.id2': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GEO.id               object\n",
       "GEO.id2              object\n",
       "GEO.display-label    object\n",
       "HD01_S001             int64\n",
       "HD02_S001            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 1792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf12010.dtypes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentages (e.g., HC01_V01 in 2000, HD02_S001 in 2010) are being specified as objects. I will need to fix that but since there are 195 rows in this dataframe and I am not keeping all of them, I will do it at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, read in the metadata for the census so I know what the column headers mean and which ones I want. Creating the column in the features20XXdf tables had to be done manually since the unique header names (e.g., HC01_V01) are different between the 2000 and 2010 census for the same data. For example, total population in the 2000 census data is HC01_VC01 but HD01_S001 in the 2010 census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1793,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2000 = pd.read_csv(\"data/census data/aff_download SF1 table/DEC_00_SF1_DP1_metadata_withchoosenfeatures.csv\")\n",
    "features2010 = pd.read_csv(\"data/census data/aff_download SF1 table/DEC_10_SF1_SF1DP1_metadata_withchosenfeatures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1794,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the census data to just the variables that we care about. \n",
    "namestomatch2000 = ['GEO.id', 'GEO.id2'] +  features2000.iloc[:,0].values.tolist()\n",
    "namestomatch2010 = ['GEO.id', 'GEO.id2'] +  features2010.iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1795,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset 2000 and 2010 american family census data to just the features that I want.\n",
    "amf2000 = sf1_2000.loc[ : , namestomatch2000]\n",
    "amf2010 = sf1_2010.loc[ : , namestomatch2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1796,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to rename columns to more intuitive headers\n",
    "newnames2000 = ['GEO.id', 'zipcode'] +  features2000.iloc[:,3].values.tolist()\n",
    "newnames2010 = ['GEO.id', 'zipcode'] +  features2010.iloc[:,3].values.tolist()\n",
    "\n",
    "dictionary2000 = dict(zip(namestomatch2000, newnames2000))\n",
    "dictionary2010 = dict(zip(namestomatch2010, newnames2010))\n",
    "\n",
    "amf2000.rename(columns = dictionary2000, inplace=True)\n",
    "amf2010.rename(columns = dictionary2010, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in percent rural and percent urban from the other census data file. \n",
    "# Drop the 1st row that has descriptors and units, and just get the 5 columns I need.\n",
    "urban2000 = pd.read_csv(\"data/census data/population urban vs rural/DEC_00_SF1_P002_with_ann.csv\",\n",
    "                        skiprows=[1], usecols = [0,1,3,4], dtype={'GEO.id2': object})\n",
    "urban2010 = pd.read_csv(\"data/census data/population urban vs rural/DEC_10_SF1_P2_with_ann.csv\",\n",
    "                        skiprows=[1], usecols = [0,1,3,4], dtype={'GEO.id2': object})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just want to know the percent of the total population that is urban vs. rural. Since one of those, urban or rural, gives me all the info I need I will just use percent urban."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1798,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns to make them more intuitive.\n",
    "urban2000.columns = ['GEO.id', 'zipcode', 'total', 'urban']\n",
    "urban2010.columns = ['GEO.id', 'zipcode', 'total', 'urban']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1799,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate percent of population to 1 decimal place. \n",
    "def percentpop(x,y):\n",
    "    if y == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return round(100*(x/y) ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1800,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percent of total population that lives in an urban area. decimal place = 1 to match other percentages in the census data. \n",
    "urban2000['population_percenturban'] = np.vectorize(percentpop)(urban2000['urban'], urban2000['total'])\n",
    "urban2010['population_percenturban'] = np.vectorize(percentpop)(urban2010['urban'], urban2010['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1801,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset urban2000 and urban2010 to just the identifiers and the percent urban column.\n",
    "urban2000 = urban2000.loc[:, ['GEO.id', 'zipcode', 'population_percenturban']]\n",
    "urban2010 = urban2010.loc[:, ['GEO.id', 'zipcode', 'population_percenturban']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1802,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge percent urban with the rest of the census data for 2000 and 2010.\n",
    "census2000 = pd.merge(amf2000, urban2000, on=[\"GEO.id\" , \"zipcode\"])\n",
    "census2010 = pd.merge(amf2010, urban2010, on=[\"GEO.id\" , \"zipcode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1803,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop GEO.id\n",
    "census2000 = census2000.loc[:, 'zipcode': ]\n",
    "census2010 = census2010.loc[:, 'zipcode': ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1804,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify all columns except zipcode as numeric.     \n",
    "for col in  census2000.columns[1:]:\n",
    "    census2000[col] = pd.to_numeric(census2000[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1805,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in  census2010.columns[1:]:\n",
    "    census2010[col] = pd.to_numeric(census2010[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of additional zipcodes that I don't want to include (I have learned more about US zipcodes than I ever thought I could over the course of this project!). https://cber.cba.ua.edu/asdc/zip_zcta.html\n",
    "\n",
    "I need to get rid of rows:\n",
    "1. without zipcode info, or with zipcodes less than 5 digits.\n",
    "2. with zip 00000 or 99999 (neither of which exist)\n",
    "3. with 5 digits zips starting with 00xxx. There represent Puerto Rico, US territories and other locations outside the US like embassies.\n",
    "4. with zip codes that have HH or XX as the last two digits. These are areas with little settlement; for example, parks, forest lands, and desert and mountainous areas where the USPS doesn't deliver mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1806,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a pandas dataframe with column 'zipcode'\n",
    "def zipcodeclean(x):\n",
    "    return x[(x['zipcode']!= \"00000\") \n",
    "                        & (x['zipcode']!= \"99999\") \n",
    "                        & ~(x.zipcode.str.len() < 5) \n",
    "                        & (x.zipcode.str.isnumeric() == True)\n",
    "                        & (x.zipcode.str[0:2] != \"00\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "metadata": {},
   "outputs": [],
   "source": [
    "census2000 = zipcodeclean(census2000)\n",
    "census2010 = zipcodeclean(census2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# IRS data cleaning\n",
    "\n",
    "### 2008\n",
    "The 2008 IRS data is formatted differently than the other years so I am going to clean it separately and then combine it. Specifically, the 'agi_stubs' columns need to be aggregated for every zipcode to match the format of the other years that do not have data broken out by agi_stub (which is adjusted gross income classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs2008 = pd.read_csv(\"data/income_by_zip/08zpall.csv\", dtype={'state': object, 'ZIPCODE': object})\n",
    "irs2008.columns = irs2008.columns.str.lower() # convert all column names to lowercase. \n",
    "irs2008 = irs2008.rename(columns = {'agi_class':'agi_stub'}) # rename this column to match the other irs years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs2008aggregated = irs2008.groupby(['state', 'zipcode'], as_index = False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "metadata": {},
   "outputs": [],
   "source": [
    "colswanted = ['state'\n",
    "              , 'zipcode'\n",
    "              , 'agi_stub'\n",
    "              , 'n1' # number of returns; proxy for number of households\n",
    "              , 'n2' # number of exemptions; proxy for population\n",
    "              , 'numdep' # number of dependents\n",
    "              , 'a00100' # Adjusted gross income (AGI)\n",
    "              , 'a01700' # Pensions and annuities in AGI (might be indicative of older population)\n",
    "              , 'a18425' # State and Local Income taxes paid\n",
    "              , 'a18450' # State and local general sales taxes paid\n",
    "              , 'a18500' # Real estate taxes paid\n",
    "              , 'a18300' # Total taxes paid\n",
    "              , 'af5695' # Residential energy tax credit; proxy for beliefs on environmental policy or causes?\n",
    "              , 'a07220' # Child tax credit\n",
    "              , 'a06500' # Income tax \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs2008fixed = irs2008aggregated.loc[ : , colswanted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the dollar amounts for each zipcode so they are in 1000's of dollars and make this value be the average for each household in a given zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageperhousehold(x, y):\n",
    "    return round((x/1000)/y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1813,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in  irs2008fixed.columns[6:]:\n",
    "    irs2008fixed[col] = np.vectorize(averageperhousehold)(irs2008fixed[col], irs2008fixed['n1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, turn the number of dependents into average number of dependents per household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1814,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs2008fixed.numdep = round(irs2008fixed.numdep/irs2008fixed.n1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in a year column, and rename \"af5695\" to \"a07260\" to match the other years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1815,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs2008fixed = irs2008fixed.rename(columns = {'af5695':'a07260'})\n",
    "irs2008fixed['year'] = 2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 \n",
    "Needs to have leading zeros added to the eastern state zipcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1816,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs2011 = pd.read_csv('data/income_by_zip/11zpallagi.csv', dtype={'STATE': object, 'zipcode': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1817,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a leading zero to all zipcodes and then substring to the last 5 characters\n",
    "#irs2011.zipcode = '0' + irs2011.zipcode.astype(str)\n",
    "irs2011.zipcode = irs2011.zipcode.apply(lambda x: x[len(x)-5:len(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is fixed write it as a csv so I don't have to worry about it down the road again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1818,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv =  irs2011.to_csv ('data/income_by_zip/11zpallagi.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2009 - 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1819,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fname = glob.glob(\"data/income_by_zip/*pallagi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1820,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/income_by_zip/12zpallagi.csv',\n",
       " 'data/income_by_zip/11zpallagi.csv',\n",
       " 'data/income_by_zip/14zpallagi.csv',\n",
       " 'data/income_by_zip/13zpallagi.csv',\n",
       " 'data/income_by_zip/09zpallagi.csv',\n",
       " 'data/income_by_zip/10zpallagi.csv']"
      ]
     },
     "execution_count": 1820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1821,
   "metadata": {},
   "outputs": [],
   "source": [
    "def irsclean(x):\n",
    "    colswanted = [ 'year'\n",
    "                  ,'state'\n",
    "                  , 'zipcode'\n",
    "                  , 'agi_stub'\n",
    "                  , 'n1' # number of returns; proxy for number of households\n",
    "                  , 'n2' # number of exemptions; proxy for population\n",
    "                  , 'numdep' # number of dependents\n",
    "                  , 'a00100' # Adjusted gross income (AGI)\n",
    "                  , 'a01700' # Pensions and annuities in AGI (might be indicative of older population)\n",
    "                  , 'a18425' # State and Local Income taxes paid\n",
    "                  , 'a18450' # State and local general sales taxes paid\n",
    "                  , 'a18500' # Real estate taxes paid\n",
    "                  , 'a18300' # Total taxes paid\n",
    "                  , 'a07260' # Residential energy tax credit; proxy for beliefs on environmental policy or causes?\n",
    "                  , 'a07220' # Child tax credit\n",
    "                  , 'a06500' # Income tax \n",
    "                 ]\n",
    "    tmp = pd.read_csv(x, dtype={'STATE': object, 'zipcode': object})\n",
    "    tmp['year'] = '20'+ x[19:21] # get the 4 digit year for each irs csv file from the file name.\n",
    "    tmp.columns = tmp.columns.str.lower() # make all column names lower case\n",
    "    tmp = tmp.loc[ : , colswanted]\n",
    "      \n",
    "    return tmp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1822,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs09to14 = []\n",
    "for i in range(0, len(fname)-1):\n",
    "    data = irsclean(fname[i])\n",
    "    # store DataFrame in list\n",
    "    irs09to14.append(data)\n",
    "irs09to14 = pd.concat(irs09to14, axis=0) # concat to a single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1823,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the column data types to they are all numbers\n",
    "#for col in irs09to14.columns[6:]:\n",
    "#    irs09to14[col] = np.vectorize(averageperhousehold)(irs09to14[col], irs09to14['n1'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the number of dependents to be average per household in each zip. This is done separately from the other columns above because I am using a different number of significant digits to match the census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1824,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs09to14.numdep = round(irs09to14.numdep/irs09to14.n1, 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1825,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the 2008 columns in the same order as irs09to14\n",
    "yy = list(irs09to14.columns)\n",
    "irs2008fixed = irs2008fixed[yy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1826,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs = pd.concat([irs2008fixed,irs09to14], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1827,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs = zipcodeclean(irs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate irs for each zipcode to election cycle by averaging the values for each 2 year period (e.g. 2010 election cycle = 2009 and 2010 data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in fecyear\n",
    "#irs['fecyear'] = 2 * np.ceil(irs.year.astype('int')/2)\n",
    "#irs['fecyear'] = irs.fecyear.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1829,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x here is a year column from a pd.df\n",
    "def makefecyear(x):\n",
    "    tmp =  round(2 * np.ceil(x.astype(str).astype(int)/2))\n",
    "    return tmp.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1830,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs['fecyear'] = makefecyear(irs.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1831,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year\n",
    "irs = irs.drop(['year'], axis = 1)\n",
    "# aggregate by fecyear\n",
    "irsbyfeccyle = irs.groupby(['fecyear', 'state', 'zipcode'], as_index = False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge census and irs data\n",
    "The 2000 census data gets merged with irs.fecyear = 2008, and all other irs feccycles are merged with 2010 census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1832,
   "metadata": {},
   "outputs": [],
   "source": [
    "irsfec2008 = irsbyfeccyle[(irsbyfeccyle['fecyear'] == 2008)]\n",
    "censusirs2008 =  pd.merge(census2000, irsfec2008, on = 'zipcode', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1833,
   "metadata": {},
   "outputs": [],
   "source": [
    "irsfec2010on = irsbyfeccyle[(irsbyfeccyle['fecyear'] != 2008)]\n",
    "censusirs2010on =  pd.merge(census2010, irsfec2010on, on = 'zipcode', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1834,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns to match\n",
    "yy = list(censusirs2010on.columns)\n",
    "censusirs2008 = censusirs2008[yy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1835,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_irs = pd.concat([censusirs2008,censusirs2010on], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEC data cleaning\n",
    "\n",
    "This is going to use SQLite to pull in and doing some of the data pre-processing. The FEC donor database from 2007-2014 for the entire US was approximately 11 million rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1836,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/database_Jun9.db') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1837,
   "metadata": {},
   "outputs": [],
   "source": [
    "uquery = \"\"\"SELECT SUBSTR(m.ZIP_CODE, 1,5) as zipcode, \n",
    "CAND_PTY_AFFILIATION as party, SUBSTR(TRANSACTION_DT,-4) as year, \n",
    "SUM(TRANSACTION_AMT) as amt, SUM(1) as ndon \n",
    "FROM (SELECT CMTE_ID, ZIP_CODE, TRANSACTION_DT,\n",
    "(CASE\n",
    "    WHEN TRANSACTION_AMT > 5400 THEN 5400 \n",
    "    ELSE TRANSACTION_AMT END) TRANSACTION_AMT \n",
    "    FROM contribs \n",
    "    WHERE TRANSACTION_AMT >= 200\n",
    "    ) m \n",
    "JOIN (SELECT DISTINCT CAND_ID, CMTE_ID FROM links) l ON m.CMTE_ID == l.CMTE_ID \n",
    "JOIN (SELECT DISTINCT CAND_ID, CAND_NAME, \n",
    "CASE upper(CAND_PTY_AFFILIATION) WHEN 'DEM' THEN 'DEM' WHEN 'REP' THEN 'REP' ELSE 'THR' END CAND_PTY_AFFILIATION\n",
    "    FROM cand_4) c ON c.CAND_ID = l.CAND_ID \n",
    "GROUP BY 1, 2, 3;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1838,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs = pd.read_sql_query(uquery, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1839,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns where year is blank or year < 2004 or year > 2015, year is an object in this df\n",
    "lis = range(2005,2014)\n",
    "yearswanted = [\"{:02d}\".format(x) for x in lis]\n",
    "contribs = contribs[(contribs['year'].isin(yearswanted))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1840,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop row without zipcodes,from Puerto Rico, and outside US (territories, embassies etc.) start with 00.\n",
    "contribs = zipcodeclean(contribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1841,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in FEC year\n",
    "contribs['fecyear'] = makefecyear(contribs.year) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1842,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year\n",
    "contribs = contribs.drop(['year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1843,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribsbyFEC = contribs.groupby(['fecyear', 'zipcode', 'party'], as_index = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every zipcode also get the amount and ndon from last election cycle. Basically take the FEC data and offset if by one cycle then remerge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1844,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of contribsbyFEC and rename columns \n",
    "feclast = contribsbyFEC.copy()\n",
    "feclast = feclast.rename(columns = {'fecyear': 'lastfecyear', 'amt': 'amtlastcycle', 'ndon': 'ndonlastcycle' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake the fecyear column for the merge.\n",
    "feclast['fecyear'] = feclast.lastfecyear.astype(int) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1846,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with the contribsbyFEC by fecyear to get the offsets correct.\n",
    "fecwithlast =  pd.merge(contribsbyFEC, feclast, on =['fecyear', 'zipcode', 'party'], how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1847,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the lastfecyear column as it was only needed for the merge.\n",
    "fecwithlast = fecwithlast.drop(['lastfecyear'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1848,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 2006 election cycle as is was only included to create these last cycle amt and ndon colums, fecyear 2016\n",
    "fecwithlast = fecwithlast[(fecwithlast['fecyear'] != 2006) & (fecwithlast['fecyear'] != 2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1849,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if amt or ndon last cycle is an NaN then set to zero.\n",
    "values = {'amtlastcycle': 0, 'ndonlastcycle': 0}\n",
    "fec = fecwithlast.fillna(value = values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the data cleaning was completed in R to take advantage of some geospatial packages for imputing missing values of target features based on averages of proximal zipcodes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
